{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, handle our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare a function, clean_comment, to regex and tokenize comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_comment(comment):\n",
    "    ps = PorterStemmer()\n",
    "    regex = re.compile('[^ a-zA-Z]')\n",
    "    cleaned_comment = regex.sub('', comment)\n",
    "    tokenized_words = word_tokenize(cleaned_comment.lower())\n",
    "    cleaned_comments = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_comments.append(ps.stem(word))\n",
    "\n",
    "    return cleaned_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a df of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stock_data.csv', sep=',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look into class balance in the above df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = None)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Sentiment\", data=df)\n",
    "plt.title(\"Pos vs. Negative Sentiment\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, we need to upsample our negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df[df['Sentiment'] == 1]\n",
    "df_minority = df[df['Sentiment'] == -1]\n",
    "\n",
    "minority_upsample = resample(df_minority, replace = True, n_samples = df_majority.shape[0], random_state=101)\n",
    "\n",
    "df_upsampled = pd.concat([minority_upsample, df_majority])\n",
    "df_upsampled = df_upsampled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = None)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Sentiment\", data=df_upsampled)\n",
    "plt.title(\"Pos vs. Negative Sentiment\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Message Counter & Occurence Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurrence(frequency, word, label):\n",
    "    '''\n",
    "    Params:\n",
    "        frequency: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Return:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    composite_key = (word, label)\n",
    "    n = frequency[composite_key]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_counter(output_occurrence, comments, sentiment):\n",
    "    '''\n",
    "    Params:\n",
    "        output_occurrence: a dictionary that will be used to map each pair to its frequency\n",
    "        messages: a list of messages\n",
    "        spam_or_ham: a list corresponding to the sentiment of each message (either 0 or 1)\n",
    "    Return:\n",
    "        output: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ## Steps :\n",
    "    # define the key, which is the word and label tuple\n",
    "    # if the key exists in the dictionary, increment the count\n",
    "    # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "\n",
    "    output_occurence = {}\n",
    "\n",
    "\n",
    "    for label, comment in zip(sentiment, comments):\n",
    "        for word in clean_comment(comment):\n",
    "            composite_key = (word, label)\n",
    "            keys = output_occurrence.keys()\n",
    "            if composite_key in keys:\n",
    "                output_occurrence[composite_key] += 1\n",
    "            else:\n",
    "                output_occurrence[composite_key] = 1\n",
    "\n",
    "    return output_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_upsampled['Text']\n",
    "sentiment = df_upsampled['Sentiment']\n",
    "frequencies = comment_counter(output_occurrence={}, comments=comments, sentiment=sentiment)\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Naive Bayes Function. This implementation is from A1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(frequencies, comments, sentiment):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of messages\n",
    "        train_y: a list of labels correponding to the messages (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    vocab = []\n",
    "   \n",
    "    for key in frequencies.keys():\n",
    "        vocab.append(key[0])\n",
    "\n",
    "    # calculate num_pos and num_neg - the total number of positive and negative words for all documents\n",
    "    num_pos = num_neg = 0\n",
    "    for pair in frequencies.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if frequencies[pair] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            num_pos += frequencies[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            num_neg += frequencies[pair]\n",
    "\n",
    "    # Calculate num_doc, the number of documents\n",
    "    num_doc = len(frequencies)  \n",
    "\n",
    "    # Calculate D_pos, the number of positive documents \n",
    "    pos_num_docs = 0\n",
    "    for pair in frequencies.keys():\n",
    "        if frequencies[pair] > 0:\n",
    "            pos_num_docs += 1\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents \n",
    "    neg_num_docs = 0\n",
    "    for pair in frequencies.keys():\n",
    "        if frequencies[pair] > 0:\n",
    "            neg_num_docs += 1\n",
    "\n",
    "    # Calculate logprior\n",
    "    positive_sentiment = 0\n",
    "    negative_sentiment = 0\n",
    "    for num in sentiment:\n",
    "        if num > 0:\n",
    "            positive_sentiment += 1\n",
    "    else:\n",
    "        negative_sentiment += 1\n",
    "\n",
    "    logprior = math.log(pos_num_docs) - math.log(neg_num_docs)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        pos_key = (word, 1)\n",
    "        neg_key = (word, 0)\n",
    "        freq_pos = 0\n",
    "        freq_neg = 0\n",
    "        # get the positive and negative frequency of the word\n",
    "        if pos_key in frequencies.keys():\n",
    "            freq_pos = frequencies[pos_key]\n",
    "\n",
    "        if neg_key in frequencies.keys():\n",
    "            freq_neg = frequencies[neg_key]\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1)/(positive_sentiment+num_doc)\n",
    "        p_w_neg = (freq_neg + 1)/(negative_sentiment+num_doc)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = math.log(p_w_pos/p_w_neg)\n",
    "\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(frequencies, comments, sentiment)\n",
    "#print(logprior)\n",
    "#print(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented Naive Bayes, we should test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Insert test here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement our naive bayes predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(comment, logprior, loglikelihood):\n",
    "    cleaned_comment = clean_comment(comment)\n",
    "    sentiment = 0\n",
    "    sentiment += logprior\n",
    "    for word in cleaned_comment:\n",
    "        if word in loglikelihood.keys():\n",
    "            sentiment += loglikelihood[word]\n",
    "    \n",
    "    if sentiment > 0:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = -1\n",
    "    \n",
    "    return sentiment\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply this to the WSB comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('wsb_comments.txt', 'r')\n",
    "comments = text.readlines()\n",
    "rated_comments = {}\n",
    "\n",
    "for comment in comments:\n",
    "    sentiment = naive_bayes_predict(comment, logprior, loglikelihood)\n",
    "    rated_comments[comment] = sentiment\n",
    "\n",
    "\n",
    "print(rated_comments)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
