{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare a function, clean_comment, to regex and tokenize comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_comment(comment):\n",
    "    ps = PorterStemmer()\n",
    "    regex = re.compile('[^ a-zA-Z]')\n",
    "    cleaned_comment = regex.sub('', comment)\n",
    "    tokenized_words = word_tokenize(cleaned_comment.lower())\n",
    "    cleaned_comments = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_comments.append(ps.stem(word))\n",
    "\n",
    "    return cleaned_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a df of our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stock_data.csv', sep=',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look into class balance in the above df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = None)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Sentiment\", data=df)\n",
    "plt.title(\"Pos vs. Negative Sentiment\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, we need to upsample our negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df[df['Sentiment'] == 1]\n",
    "df_minority = df[df['Sentiment'] == -1]\n",
    "\n",
    "minority_upsample = resample(df_minority, replace = True, n_samples = df_majority.shape[0], random_state=101)\n",
    "\n",
    "df_upsampled = pd.concat([minority_upsample, df_majority])\n",
    "df_upsampled = df_upsampled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = None)\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x=\"Sentiment\", data=df_upsampled)\n",
    "plt.title(\"Pos vs. Negative Sentiment\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We begin by compiling a feature list of words from our positive and negative comments to begin to see trends in which words fall into which category generally.\n",
    "\n",
    "We begin by building a frequency distribution of words in our comments. We can also use this to build our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def word_counter(comments, sentiment):\n",
    "    '''\n",
    "    Params:\n",
    "        comments: a list of comments\n",
    "        sentiment: a list corresponding to the sentiment of each message (either 0 or 1)\n",
    "    Return:\n",
    "        output_occurence: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "    output_occurence = {}\n",
    "    vocab = []\n",
    "\n",
    "\n",
    "    for label, comment in zip(sentiment, comments):\n",
    "        for word in clean_comment(comment):\n",
    "            vocab.append(word)\n",
    "            composite_key = (word, label)\n",
    "            keys = output_occurence.keys()\n",
    "            if composite_key in keys:\n",
    "                output_occurence[composite_key] += 1\n",
    "            else:\n",
    "                output_occurence[composite_key] = 1\n",
    "\n",
    "    vocab = set(vocab)\n",
    "    \n",
    "    return output_occurence, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(documents)\n",
    "#print(documents[0])\n",
    "vocab = []\n",
    "comments = []\n",
    "sentiments = []\n",
    "documents = []\n",
    "\n",
    "for comment in df_upsampled['Text']:\n",
    "    comments.append(clean_comment(comment))\n",
    "\n",
    "for sentiment in df_upsampled['Sentiment']:\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "for i in range(len(comments)):\n",
    "    documents.append((comments[i], sentiments[i]))\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "for comment in comments:\n",
    "    for word in comment:\n",
    "        vocab.append(word)\n",
    "\n",
    "vocab = nltk.FreqDist(vocab)\n",
    "\n",
    "vocab_features = list(vocab.keys())\n",
    "\n",
    "print(documents[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we categorize each word as positive or negative. We build a function that finds words in our comments and gets their most frequent classification from the dictionary created in find_occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(comment):\n",
    "    '''\n",
    "    Params:\n",
    "        document: our comment and sentiment\n",
    "    Return:\n",
    "        the features, a dictionary of words in the comment mapped to sentiment\n",
    "    '''\n",
    "    words = comment\n",
    "    features = {}\n",
    "\n",
    "    for w in vocab_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(comment), sentiment) for (comment, sentiment) in documents]\n",
    "\n",
    "print(featuresets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how this works on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = round(len(featuresets) * 0.8)\n",
    "training_set = featuresets[:cutoff]\n",
    "testing_set = featuresets[cutoff+1:]\n",
    "\n",
    "naive_bayes_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(naive_bayes_classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our most influential words below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we have an accuracy of 82%! Now, we should save this classifier to use when voting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open('naivebayes.pickle','wb')\n",
    "pickle.dump(naive_bayes_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other classifiers - this may take a substantial amount of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy of the above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "print(\"BNB accuracy percent:\", (nltk.classify.accuracy(BNB_classifier, testing_set))*100)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration of our vote classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassifierI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chadcomiter/Documents/CS6120 NLP/NLP_Final_Project_Group_20/Sentiment_Analyzer.ipynb Cell 30'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chadcomiter/Documents/CS6120%20NLP/NLP_Final_Project_Group_20/Sentiment_Analyzer.ipynb#ch0000030?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mVotingClassifier\u001b[39;00m(ClassifierI):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chadcomiter/Documents/CS6120%20NLP/NLP_Final_Project_Group_20/Sentiment_Analyzer.ipynb#ch0000030?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mclassifiers):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chadcomiter/Documents/CS6120%20NLP/NLP_Final_Project_Group_20/Sentiment_Analyzer.ipynb#ch0000030?line=2'>3</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_classifiers \u001b[39m=\u001b[39m classifiers\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassifierI' is not defined"
     ]
    }
   ],
   "source": [
    "class VotingClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "        def classify(self, features):\n",
    "            votes = []\n",
    "            for classifier in self._classifiers:\n",
    "                vote = classifier.classify(features)\n",
    "                votes.append(vote)\n",
    "            return mode(votes)\n",
    "\n",
    "        def evaluate_confidence(self, features):\n",
    "            votes = []\n",
    "            for classifier in self._classifiers:\n",
    "                vote = classifier.classify(features)\n",
    "                votes.append(vote)\n",
    "            \n",
    "            choice_votes = votes.count(mode(votes))\n",
    "            confidence = choice_votes / len(votes)\n",
    "            return confidence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier = VotingClassifier(naive_bayes_classifier, MNB_classifier, BNB_classifier, LogisticRegression_classifier, SGDClassifier_classifier, SVC_classifier, LinearSVC_classifier, NuSVC_classifier)\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voting_classifier, testing_set))*100)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
